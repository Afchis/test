{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testcv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vrPNWEC2Qej5K134F2W92SvPQTbhTltf",
      "authorship_tag": "ABX9TyOZluwtIKvSyghRa2ePTZWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afchis/test/blob/main/testcv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnVY2EvhEyCr"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "!wget --random-wait -P ./data_georg/non_georges/ -i ./drive/MyDrive/test_assignment_cv_engineer_data/non_georges.csv \r\n",
        "!wget --random-wait -P ./data_georg/georges/ -i ./drive/MyDrive/test_assignment_cv_engineer_data/georges.csv "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOcXta5lgtjR"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "\r\n",
        "to_pill = transforms.ToPILImage()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEkm22W-vq-L"
      },
      "source": [
        "# Args:\r\n",
        "IMG_SIZE = 224\r\n",
        "BATCH_SIZE = 16\r\n",
        "NUM_WORKERS = 8\r\n",
        "SPLIT_DATA = 0.8\r\n",
        "LEANING_RATE = 0.005\r\n",
        "EPOCHS = 20\r\n",
        "\r\n",
        "LABELS = {0 : \"george\",\r\n",
        "          1 : \"non_george\"}"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmUEFh_NiJ1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39129d1b-c422-4173-9506-1c1de33db81d"
      },
      "source": [
        "# Data Preparation:\r\n",
        "transform = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=0),\r\n",
        "                                transforms.RandomHorizontalFlip(p=0.5),\r\n",
        "                                transforms.RandomRotation(45),\r\n",
        "                                transforms.ToTensor()])\r\n",
        "\r\n",
        "data = datasets.ImageFolder(os.path.join(\"/content/data_georg\"), transform=transform)\r\n",
        "\r\n",
        "\r\n",
        "train_size = int(SPLIT_DATA * len(data))\r\n",
        "valid_size = len(data) - train_size\r\n",
        "train_data, valid_data = torch.utils.data.random_split(data, [train_size, valid_size])\r\n",
        "\r\n",
        "valid_size = int(SPLIT_DATA * valid_size)\r\n",
        "test_size = len(data) - train_size - valid_size\r\n",
        "valid_data, test_data = torch.utils.data.random_split(valid_data, [valid_size, test_size])\r\n",
        "\r\n",
        "print(\"Full dataset len: \", len(data))\r\n",
        "print(\"Train dataset len: \" , len(train_data))\r\n",
        "print(\"Valid dataset len: \", len(valid_data))\r\n",
        "print(\"Test dataset len\", len(test_data))\r\n",
        "\r\n",
        "train_loader = DataLoader(dataset=train_data,\r\n",
        "                          batch_size=BATCH_SIZE,\r\n",
        "                          num_workers=NUM_WORKERS,\r\n",
        "                          shuffle=True)\r\n",
        "valid_loader = DataLoader(dataset=valid_data,\r\n",
        "                          batch_size=BATCH_SIZE,\r\n",
        "                          num_workers=NUM_WORKERS,\r\n",
        "                          shuffle=False)\r\n",
        "test_loader = DataLoader(dataset=test_data,\r\n",
        "                         batch_size=1,\r\n",
        "                         num_workers=NUM_WORKERS,\r\n",
        "                         shuffle=False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full dataset len:  5700\n",
            "Train dataset len:  4560\n",
            "Valid dataset len:  912\n",
            "Test dataset len 228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpNKIldE6WrH"
      },
      "source": [
        "# Create model\r\n",
        "class ResNet50(nn.Module):\r\n",
        "    def __init__(self, pretrained=False):\r\n",
        "        super().__init__()\r\n",
        "        self.resnet50 = models.resnet50(pretrained=pretrained)\r\n",
        "        self.resnet50_layers = list(self.resnet50.children())\r\n",
        "        self.cnn = nn.Sequential(*self.resnet50_layers[:-1])\r\n",
        "        self.fc = nn.Linear(in_features=2048, out_features=1, bias=True)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "      \r\n",
        "    def forward(self, x):\r\n",
        "        out = self.cnn(x)\r\n",
        "        out = self.fc(out.reshape(out.size(0), -1))\r\n",
        "        out = self.sigmoid(out)\r\n",
        "        return out"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOe-XsfczoKS"
      },
      "source": [
        "# Init model, optimizer, scheluler\r\n",
        "model = ResNet50(pretrained=True).cuda()\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEANING_RATE)\r\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\r\n",
        "\r\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZL_IegI3lyB"
      },
      "source": [
        "def train():\r\n",
        "    for epoch in range(EPOCHS):\r\n",
        "        model.train()\r\n",
        "        loss_acc = 0\r\n",
        "        for iter, data in enumerate(train_loader):\r\n",
        "            iter += 1\r\n",
        "            img, label = data\r\n",
        "            img, label = img.cuda(), label.float().cuda()\r\n",
        "            out = model(img)\r\n",
        "            loss = criterion(out, label.reshape(out.size()))\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            loss_acc += loss.item()\r\n",
        "            loss_train = loss_acc / iter\r\n",
        "            print(\" \"*70, end=\"\\r\")\r\n",
        "            print(\"Process ...\", \"Train iter:\", iter, \"loss:\", loss_train, end=\"\")\r\n",
        "        model.eval()\r\n",
        "        loss_acc = 0\r\n",
        "        for iter, data in enumerate(valid_loader):\r\n",
        "            iter += 1\r\n",
        "            img, label = data\r\n",
        "            img, label = img.cuda(), label.float().cuda()\r\n",
        "            out = model(img)\r\n",
        "            loss = criterion(out, label.reshape(out.size()))\r\n",
        "            loss_acc += loss.item()\r\n",
        "            loss_valid = loss_acc / iter\r\n",
        "            print(\" \"*70, end=\"\\r\")\r\n",
        "            print(\"Process ...\", \"Valid iter:\", iter, \"loss:\", loss_valid, end=\"\")\r\n",
        "        scheduler.step()\r\n",
        "        print(\" \"*70, end=\"\\r\")\r\n",
        "        print(\"Epoch:\", epoch, \"Train loss:\", loss_train, \"Valid loss:\", loss_valid)\r\n",
        "\r\n",
        "\r\n",
        "def test():\r\n",
        "    correct = 0\r\n",
        "    for iter, data in enumerate(test_loader):\r\n",
        "        iter += 1\r\n",
        "        img, label = data\r\n",
        "        img, label = img.cuda(), label.float().cuda()\r\n",
        "        out = model(img)\r\n",
        "        pred = (out>0.5).float()\r\n",
        "        if pred == label: \r\n",
        "            correct += 1.\r\n",
        "    accuracy = correct / iter\r\n",
        "    print(\"Model acсuracy: %0.2f\" % (accuracy*100))\r\n",
        "\r\n",
        "\r\n",
        "def inference(idx, labels=LABELS, test_data=test_data):\r\n",
        "    pred = model(test_data[idx][0].unsqueeze(0).cuda())\r\n",
        "    print(labels[(pred > 0.5).item()])\r\n",
        "    # return to_pill(test_data[idx][0])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7H4hqkM58qA",
        "outputId": "29d8bbbd-08c5-44d6-d193-1f1f1474faaf"
      },
      "source": [
        "train()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train loss: 0.4957988835740508 Valid loss: 0.36974116663138074\n",
            "Epoch: 1 Train loss: 0.3353715497150756 Valid loss: 0.3032502784279355\n",
            "Epoch: 2 Train loss: 0.27026416324732594 Valid loss: 0.26769989245293435\n",
            "Epoch: 3 Train loss: 0.2393423610089118 Valid loss: 0.25256026797650155\n",
            "Epoch: 4 Train loss: 0.21504985377715344 Valid loss: 0.24330604475056916\n",
            "Epoch: 5 Train loss: 0.18959443934523224 Valid loss: 0.24036219075583576\n",
            "Epoch: 6 Train loss: 0.187611606501435 Valid loss: 0.23423817747256212\n",
            "Epoch: 7 Train loss: 0.1615837683574411 Valid loss: 0.23853310436141073\n",
            "Epoch: 8 Train loss: 0.1641975353030782 Valid loss: 0.22662079840767801\n",
            "Epoch: 9 Train loss: 0.15383789247195973 Valid loss: 0.2394308867423158\n",
            "Epoch: 10 Train loss: 0.14900961628739248 Valid loss: 0.22489245128082602\n",
            "Epoch: 11 Train loss: 0.1317476462507457 Valid loss: 0.241335317623197\n",
            "Epoch: 12 Train loss: 0.13808104912832117 Valid loss: 0.24278639907246097\n",
            "Epoch: 13 Train loss: 0.12994026850843637 Valid loss: 0.22647988845251107\n",
            "Epoch: 14 Train loss: 0.13582972128009588 Valid loss: 0.22463856356447204\n",
            "Epoch: 15 Train loss: 0.1388622973016218 Valid loss: 0.2402514336598024\n",
            "Epoch: 16 Train loss: 0.14087425240952717 Valid loss: 0.21951387394546418\n",
            "Epoch: 17 Train loss: 0.12633858271139234 Valid loss: 0.23736068694607207\n",
            "Epoch: 18 Train loss: 0.13802972491830587 Valid loss: 0.22848635630910857\n",
            "Epoch: 19 Train loss: 0.12857084887167602 Valid loss: 0.254253334545514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj51VvIAHgt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb77cfc1-9686-4aa1-b126-44fbd8ef9f01"
      },
      "source": [
        "test()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model acсuracy: 91.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQfeGBte1UJ-",
        "outputId": "6e3cb4a3-0f39-4931-c179-ab4ea7f4ed26"
      },
      "source": [
        "inference(19)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "non_george\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ncBPkC2XqPk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}